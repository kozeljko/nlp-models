{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm_glove300_sigmoid.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNGVGXw86d0dGojrOuoqnzY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kozeljko/nlp-models/blob/master/lstm_glove300_sigmoid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_kErZpcJrjS"
      },
      "source": [
        "Init environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORjKO2M5mPIR"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "d2XwduGIJj5J",
        "outputId": "2c791904-fe90-421d-c253-a1253ac6e2f8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "PuDSSVNhbgN1",
        "outputId": "ab591a06-eddc-4bf7-c845-479a7b548f41"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQHEJLoILmgY"
      },
      "source": [
        "# How I got Glove6B embeddings\n",
        "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "#!ls\n",
        "#!unzip glove.6B.zip\n",
        "#!mkdir drive/MyDrive/nlp/models-pre/glove6B\n",
        "#!mv *d.txt drive/MyDrive/nlp/models-pre/glove6B"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "2TiRSyRsJnqw",
        "outputId": "97576e30-d539-4524-9f00-a35b849ff9ff"
      },
      "source": [
        "!pip install lemmagen3 emoji\n",
        "!pip install --upgrade keras\n",
        "!pip install tensorflow-addons\n",
        "\n",
        "import sys\n",
        "sys.path.append('drive/MyDrive/nlp/nlp-offensive-language/src')\n",
        "\n",
        "\n",
        "from preprocessing import preprocess\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting lemmagen3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4d/80/b0d1f328a512fb54aa120f491f14ebba18add825908b56c3c7da7a1fe542/lemmagen3-3.3.1-cp37-cp37m-manylinux2010_x86_64.whl (12.4MB)\n",
            "\u001b[K     |████████████████████████████████| 12.4MB 243kB/s \n",
            "\u001b[?25hCollecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/fa/b3368f41b95a286f8d300e323449ab4e86b85334c2e0b477e94422b8ed0f/emoji-1.2.0-py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 39.6MB/s \n",
            "\u001b[?25hCollecting pybind11>=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/43/7339dbabbc2793718d59703aace4166f53c29ee1c202f6ff5bf8a26c4d91/pybind11-2.6.2-py2.py3-none-any.whl (191kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 41.3MB/s \n",
            "\u001b[?25hInstalling collected packages: pybind11, lemmagen3, emoji\n",
            "Successfully installed emoji-1.2.0 lemmagen3-3.3.1 pybind11-2.6.2\n",
            "Requirement already up-to-date: keras in /usr/local/lib/python3.7/dist-packages (2.4.3)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.7/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras) (1.15.0)\n",
            "Collecting tensorflow-addons\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/e3/56d2fe76f0bb7c88ed9b2a6a557e25e83e252aec08f13de34369cd850a0b/tensorflow_addons-0.12.1-cp37-cp37m-manylinux2010_x86_64.whl (703kB)\n",
            "\u001b[K     |████████████████████████████████| 706kB 4.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.12.1\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syFNIdyDQFjC"
      },
      "source": [
        "DATASETS_DIR = \"drive/MyDrive/nlp/nlp-offensive-language/datasets/\"\n",
        "\n",
        "GLOVE_DIR = \"drive/MyDrive/nlp/models-pre/glove6B/\"\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 100"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvmLp47rQPB1",
        "outputId": "91a500d1-fb84-41aa-ea38-04d8f14a4cdb"
      },
      "source": [
        "import os, csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from preprocessing import *\n",
        "\n",
        "#filename = \"english/fox_news/dataset.csv\"\n",
        "#filename = \"english/gab_and_reddit/dataset.csv\"\n",
        "#filename = \"english/deep_offense/dataset.csv\"\n",
        "#filename = \"english/trac_2/dataset.csv\"\n",
        "filename = \"english/wiki_detox/dataset_aggression.csv\"\n",
        "#filename = \"english/wiki_detox/dataset_attack.csv\"\n",
        "#filename = \"english/wiki_detox/dataset_toxicity.csv\"\n",
        "csv_read = csv.reader(open(os.path.join(DATASETS_DIR, filename), encoding=\"utf8\"), delimiter=\",\")\n",
        "\n",
        "texts = []\n",
        "labels = []\n",
        "for line in csv_read:\n",
        "  if line[0] == \"id\":\n",
        "    continue\n",
        "\n",
        "  text = line[1]\n",
        "  text = text.replace(\"NEWLINE_TOKEN\", \"\")\n",
        "  text = preprocess(text, [PP_LOWERCASE, PP_REMOVE_USERNAME_HANDLES, PP_REMOVE_URLS, PP_REMOVE_SPECIAL_CHARACTERS, PP_REMOVE_BASE_PUNCTUATIONS, PP_REMOVE_NUMBERS])\n",
        "  if (len(text) > MAX_SEQUENCE_LENGTH):\n",
        "    continue\n",
        "\n",
        "  text = \" \".join(text)\n",
        "\n",
        "  texts.append(text)\n",
        "  if \"OFF\" in line[2]:\n",
        "    labels.append([1])\n",
        "  else:\n",
        "    labels.append([0])\n",
        "\n",
        "print(str(count))\n",
        "print(\"Loaded dataset\")\n",
        "print(str(len(texts)) + \" texts\")\n",
        "print(\"First: \" + texts[0])\n",
        "print(labels[0])"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20122\n",
            "Loaded dataset\n",
            "95742 texts\n",
            "First: true or false the situation as of march was such a saudi proposal of land for peace and recognition by all arab countries was made the day the proposal was to be made formal by the arab league was the day the israeli is under the command of ariel sharon began the invasion of the palestinian selfrule areas user arab\n",
            "[0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e97MOQT9N6YS",
        "outputId": "a8dc5e87-a178-4813-dee0-7d15aa742eb7"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from keras.layers import Embedding\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Prepare tokenizer given loaded texts\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "seq = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index)+1\n",
        "\n",
        "# Load Glove model\n",
        "EMBEDDING_DIM = 300\n",
        "pad_seq = pad_sequences(seq,maxlen=MAX_SEQUENCE_LENGTH)\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(GLOVE_DIR, 'glove.6B.300d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "# Create embedding matrix\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# Create embedding layer\n",
        "\n",
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=False)\n",
        "\n",
        "# Created embedding layer"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48XboxqdsGvI"
      },
      "source": [
        "Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTJc27RdW-1T",
        "outputId": "0e231e7d-aae8-475e-fec9-bc2b960453e5"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM,Dense,Dropout,Embedding,Bidirectional\n",
        "from keras.metrics import Precision, Recall\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow_addons.metrics import F1Score\n",
        "\n",
        "\n",
        "train_index = int(len(pad_seq) * 0.9)\n",
        "\n",
        "train_seq = np.array(pad_seq[:train_index])\n",
        "train_labels = np.array(labels[:train_index])\n",
        "\n",
        "test_seq = np.array(pad_seq[train_index:])\n",
        "test_labels = np.array(labels[train_index:])\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(Dense(128,activation = 'relu'))\n",
        "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "model.add(Dense(32,activation = 'relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1,activation = 'sigmoid'))\n",
        "model.compile(optimizer='adam',loss='binary_crossentropy', metrics = [F1Score(num_classes=1, threshold=0.5), 'accuracy'])\n",
        "\n",
        "model.fit(train_seq, train_labels, epochs=10, validation_split=(1/6), batch_size=128, callbacks=[es])"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "141/141 [==============================] - 38s 225ms/step - loss: 0.4602 - f1_score: 0.2720 - accuracy: 0.8170 - val_loss: 0.3288 - val_f1_score: 0.5172 - val_accuracy: 0.8700\n",
            "Epoch 2/20\n",
            "141/141 [==============================] - 30s 212ms/step - loss: 0.3539 - f1_score: 0.5558 - accuracy: 0.8623 - val_loss: 0.3305 - val_f1_score: 0.4955 - val_accuracy: 0.8703\n",
            "Epoch 3/20\n",
            "141/141 [==============================] - 30s 213ms/step - loss: 0.3441 - f1_score: 0.5482 - accuracy: 0.8667 - val_loss: 0.3240 - val_f1_score: 0.5494 - val_accuracy: 0.8737\n",
            "Epoch 4/20\n",
            "141/141 [==============================] - 30s 213ms/step - loss: 0.3379 - f1_score: 0.5880 - accuracy: 0.8706 - val_loss: 0.3183 - val_f1_score: 0.6032 - val_accuracy: 0.8729\n",
            "Epoch 5/20\n",
            "141/141 [==============================] - 30s 215ms/step - loss: 0.3277 - f1_score: 0.6003 - accuracy: 0.8733 - val_loss: 0.3123 - val_f1_score: 0.5738 - val_accuracy: 0.8766\n",
            "Epoch 6/20\n",
            "141/141 [==============================] - 30s 215ms/step - loss: 0.3234 - f1_score: 0.6049 - accuracy: 0.8742 - val_loss: 0.3143 - val_f1_score: 0.5852 - val_accuracy: 0.8762\n",
            "Epoch 7/20\n",
            "141/141 [==============================] - 30s 214ms/step - loss: 0.3256 - f1_score: 0.6088 - accuracy: 0.8731 - val_loss: 0.3257 - val_f1_score: 0.5683 - val_accuracy: 0.8780\n",
            "Epoch 8/20\n",
            "141/141 [==============================] - 30s 215ms/step - loss: 0.3216 - f1_score: 0.6057 - accuracy: 0.8740 - val_loss: 0.3110 - val_f1_score: 0.5829 - val_accuracy: 0.8766\n",
            "Epoch 9/20\n",
            "141/141 [==============================] - 30s 214ms/step - loss: 0.3114 - f1_score: 0.6178 - accuracy: 0.8779 - val_loss: 0.3096 - val_f1_score: 0.5837 - val_accuracy: 0.8775\n",
            "Epoch 10/20\n",
            "141/141 [==============================] - 30s 215ms/step - loss: 0.3049 - f1_score: 0.6122 - accuracy: 0.8796 - val_loss: 0.3117 - val_f1_score: 0.5977 - val_accuracy: 0.8776\n",
            "Epoch 11/20\n",
            "141/141 [==============================] - 30s 215ms/step - loss: 0.2996 - f1_score: 0.6336 - accuracy: 0.8814 - val_loss: 0.3182 - val_f1_score: 0.6001 - val_accuracy: 0.8737\n",
            "Epoch 12/20\n",
            "141/141 [==============================] - 30s 215ms/step - loss: 0.2928 - f1_score: 0.6356 - accuracy: 0.8827 - val_loss: 0.3278 - val_f1_score: 0.5807 - val_accuracy: 0.8779\n",
            "Epoch 13/20\n",
            "141/141 [==============================] - 30s 215ms/step - loss: 0.2855 - f1_score: 0.6379 - accuracy: 0.8846 - val_loss: 0.3218 - val_f1_score: 0.5845 - val_accuracy: 0.8770\n",
            "Epoch 14/20\n",
            "141/141 [==============================] - 30s 214ms/step - loss: 0.2836 - f1_score: 0.6425 - accuracy: 0.8834 - val_loss: 0.3324 - val_f1_score: 0.5846 - val_accuracy: 0.8778\n",
            "Epoch 15/20\n",
            "141/141 [==============================] - 30s 215ms/step - loss: 0.2778 - f1_score: 0.6507 - accuracy: 0.8856 - val_loss: 0.3359 - val_f1_score: 0.5879 - val_accuracy: 0.8759\n",
            "Epoch 16/20\n",
            "141/141 [==============================] - 30s 214ms/step - loss: 0.2702 - f1_score: 0.6540 - accuracy: 0.8874 - val_loss: 0.3407 - val_f1_score: 0.5849 - val_accuracy: 0.8747\n",
            "Epoch 17/20\n",
            "141/141 [==============================] - 30s 214ms/step - loss: 0.2604 - f1_score: 0.6663 - accuracy: 0.8901 - val_loss: 0.3505 - val_f1_score: 0.5793 - val_accuracy: 0.8751\n",
            "Epoch 18/20\n",
            "141/141 [==============================] - 30s 215ms/step - loss: 0.2517 - f1_score: 0.6792 - accuracy: 0.8941 - val_loss: 0.3770 - val_f1_score: 0.5443 - val_accuracy: 0.8707\n",
            "Epoch 19/20\n",
            "141/141 [==============================] - 30s 215ms/step - loss: 0.2420 - f1_score: 0.6916 - accuracy: 0.8982 - val_loss: 0.3819 - val_f1_score: 0.5595 - val_accuracy: 0.8718\n",
            "Epoch 20/20\n",
            "141/141 [==============================] - 30s 215ms/step - loss: 0.2281 - f1_score: 0.7134 - accuracy: 0.9028 - val_loss: 0.4008 - val_f1_score: 0.5803 - val_accuracy: 0.8688\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f398e414290>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSQN_7CHsLRD"
      },
      "source": [
        "Evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3dOQiabJ8oJ",
        "outputId": "3e07436a-513c-481e-de99-b5b87f150432"
      },
      "source": [
        "# Evaluate\n",
        "hm = model.evaluate(test_seq, test_labels, verbose=0, return_dict=True)\n",
        "print(hm)\n",
        "#print('Test accuracy:', hm['accuracy'])\n",
        "\n",
        "TP=0\n",
        "TN=0\n",
        "FP=0\n",
        "FN=0\n",
        "total=0\n",
        "\n",
        "predictions = (model.predict(test_seq) > 0.5).astype(\"int32\")\n",
        "for i in range(len(test_seq)):\n",
        "  predicted_class = predictions[i][0]\n",
        "  actual_class = test_labels[i][0]\n",
        "\n",
        "  if actual_class == 1:\n",
        "    if predicted_class == 1:\n",
        "      TP += 1\n",
        "    else:\n",
        "      FN += 1\n",
        "  else:\n",
        "    if predicted_class == 0:\n",
        "      TN += 1\n",
        "    else:\n",
        "      FP += 1\n",
        "  \n",
        "  total += 1\n",
        "\n",
        "precision = TP / (TP + FP)\n",
        "recall = TP / (TP + FN)\n",
        "f1 = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "print(\"Precision: \" + str(precision))\n",
        "print(\"Recall: \" + str(recall))\n",
        "print(\"F1 Score: \" + str(f1))\n",
        "print(\"Accuracy: \" + str(str((TP + TN) / total)))\n"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'loss': 0.42419368028640747, 'f1_score': array([0.5672534], dtype=float32), 'accuracy': 0.8639164566993713}\n",
            "Precision: 0.6651090342679128\n",
            "Recall: 0.4944991314418066\n",
            "F1 Score: 0.5672534041846563\n",
            "Accuracy: 0.8639164490861618\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G12dGJNek9r_",
        "outputId": "3b4bf8fb-484c-4b2c-de90-ccaaa694b18a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}