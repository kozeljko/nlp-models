{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "create_multi_lstm_random_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kozeljko/nlp-models/blob/master/create_multi_lstm_random_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apjtX_WKbjiG"
      },
      "source": [
        "Notebook for building LSTM model using Glove word embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_kErZpcJrjS"
      },
      "source": [
        "Init environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORjKO2M5mPIR"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import tensorflow as tf\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2XwduGIJj5J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14a7fe0a-6ec2-49a6-ff3f-61ea684e0d38"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syFNIdyDQFjC"
      },
      "source": [
        "DATASETS_DIR = \"drive/MyDrive/nlp/nlp-offensive-language/datasets/\"\n",
        "GLOVE_DIR = \"drive/MyDrive/nlp/embeddings/glove6B/\"\n",
        "MODELS_DIR = \"drive/MyDrive/nlp/models/\"\n",
        "MODEL_NAME = \"multi_lstm_glove_en\"\n",
        "\n",
        "#LANGUAGE = \"english\"\n",
        "#DATASET = \"english/fox_news/dataset.csv\"\n",
        "#DATASET = \"english/gab_and_reddit/dataset.csv\"\n",
        "#DATASET = \"english/deep_offense/dataset.csv\"\n",
        "#DATASET = \"english/trac_2/dataset.csv\"\n",
        "#DATASET = \"english/wiki_detox/dataset_aggression.csv\"\n",
        "#DATASET = \"english/wiki_detox/dataset_attack.csv\"\n",
        "#DATASET = \"english/wiki_detox/dataset_toxicity.csv\"\n",
        "#DATASET = \"english/frenk_lgbt/dataset.csv\"\n",
        "#DATASET = \"english/frenk_migrants/dataset.csv\"\n",
        "#DATASET = \"english/combined_preprocessed/combined_dataset_train.csv\"\n",
        "\n",
        "LANGUAGE = \"slovene\"\n",
        "#DATASET = \"slovenian/frenk_migrants/dataset.csv\"\n",
        "DATASET = \"slovenian/frenk_lgbt/dataset.csv\"\n",
        "\n",
        "# Allow sequences max 100 long\n",
        "MAX_SEQUENCE_LENGTH = 100"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57iIpPjv5Gv4"
      },
      "source": [
        "Initialize preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TiRSyRsJnqw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72c97784-cf81-48fb-a79a-73499da15f09"
      },
      "source": [
        "!pip install lemmagen3 emoji\n",
        "!pip install --upgrade keras\n",
        "!pip install tensorflow-addons\n",
        "!pip install scikit-learn\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/nlp/nlp-offensive-language/src')\n",
        "\n",
        "from preprocessing import preprocess"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lemmagen3 in /usr/local/lib/python3.7/dist-packages (3.3.1)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: pybind11>=2.4 in /usr/local/lib/python3.7/dist-packages (from lemmagen3) (2.6.2)\n",
            "Requirement already up-to-date: keras in /usr/local/lib/python3.7/dist-packages (2.4.3)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.7/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.13.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYg1h45o5QbV"
      },
      "source": [
        "Load and preprocess training dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvmLp47rQPB1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f4f439c-2102-43d0-e312-1cbde820bdad"
      },
      "source": [
        "import os, csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from preprocessing import *\n",
        "\n",
        "csv_read = csv.reader(open(os.path.join(DATASETS_DIR, DATASET), encoding=\"utf8\"), delimiter=\",\")\n",
        "\n",
        "texts = []\n",
        "labels = []\n",
        "for line in csv_read:\n",
        "  if line[0] == \"id\":\n",
        "    continue\n",
        "\n",
        "  text = line[1]\n",
        "  text = text.replace(\"NEWLINE_TOKEN\", \"\")\n",
        "  text = preprocess(text, [PP_LOWERCASE, PP_REMOVE_USERNAME_HANDLES, PP_REMOVE_URLS, PP_REMOVE_SPECIAL_CHARACTERS, PP_REMOVE_BASE_PUNCTUATIONS], language=LANGUAGE)\n",
        "  if (len(text) > MAX_SEQUENCE_LENGTH):\n",
        "    continue\n",
        "\n",
        "  text = \" \".join(text)\n",
        "\n",
        "  texts.append(text)\n",
        "  labels.append(line[2])\n",
        "\n",
        "unique_labels = set(labels)\n",
        "softmax_size = len(unique_labels)\n",
        "labels_map = {}\n",
        "indexed_map = {}\n",
        "for id, label in enumerate(unique_labels):\n",
        "  # Create array represenatation for each label\n",
        "  softmax_array = [0 for i in range(softmax_size)]\n",
        "  softmax_array[id] = 1\n",
        "\n",
        "  labels_map[label] = softmax_array\n",
        "  indexed_map[id] = softmax_array\n",
        "\n",
        "# Transform labels into arrays\n",
        "labels = [labels_map[label] for label in labels]\n",
        "\n",
        "print(\"Loaded dataset\")\n",
        "print(str(len(texts)) + \" texts\")\n",
        "print(\"First: \" + texts[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded dataset\n",
            "3501 texts\n",
            "First: kako omogoča saj pa oni že lahko sklenejo zvezo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhY4558M5TMV"
      },
      "source": [
        "Create embedding layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e97MOQT9N6YS"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from keras.layers import Embedding\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Prepare tokenizer given loaded texts\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "seq = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index)+1\n",
        "\n",
        "EMBEDDING_DIM = 100\n",
        "pad_seq = pad_sequences(seq,maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Create embedding layer\n",
        "embedding_layer = Embedding(vocab_size, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48XboxqdsGvI"
      },
      "source": [
        "Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTJc27RdW-1T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cc40404-eb20-4d38-b5b8-3f6b96eeb1d3"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM,Dense,Dropout,Embedding,Bidirectional\n",
        "from keras.metrics import Precision, Recall\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "train_index = int(len(pad_seq) * 0.9)\n",
        "\n",
        "train_seq = np.array(pad_seq[:train_index])\n",
        "train_labels = np.array(labels[:train_index])\n",
        "\n",
        "test_seq = np.array(pad_seq[train_index:])\n",
        "test_labels = np.array(labels[train_index:])\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(Dense(128,activation = 'relu'))\n",
        "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "model.add(Dense(32,activation = 'relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(softmax_size ,activation = 'softmax'))\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "model.fit(train_seq, train_labels, epochs=10, validation_split=(1/6), batch_size=64, callbacks=[es])\n",
        "\n",
        "# Save model\n",
        "#model.save(os.path.join(MODELS_DIR, MODEL_NAME))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "42/42 [==============================] - 7s 69ms/step - loss: 1.4921 - accuracy: 0.4463 - val_loss: 1.3173 - val_accuracy: 0.4476\n",
            "Epoch 2/10\n",
            "42/42 [==============================] - 2s 41ms/step - loss: 1.2097 - accuracy: 0.5378 - val_loss: 1.3061 - val_accuracy: 0.4705\n",
            "Epoch 3/10\n",
            "42/42 [==============================] - 2s 41ms/step - loss: 1.0013 - accuracy: 0.6291 - val_loss: 1.4172 - val_accuracy: 0.4990\n",
            "Epoch 4/10\n",
            "42/42 [==============================] - 2s 41ms/step - loss: 0.7067 - accuracy: 0.7329 - val_loss: 1.3016 - val_accuracy: 0.4362\n",
            "Epoch 5/10\n",
            "42/42 [==============================] - 2s 41ms/step - loss: 0.7212 - accuracy: 0.7374 - val_loss: 1.6779 - val_accuracy: 0.4419\n",
            "Epoch 6/10\n",
            "42/42 [==============================] - 2s 41ms/step - loss: 0.5506 - accuracy: 0.7907 - val_loss: 1.6675 - val_accuracy: 0.4552\n",
            "Epoch 00006: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7de5d955d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSQN_7CHsLRD"
      },
      "source": [
        "Evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3dOQiabJ8oJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f097efa-52b4-4b25-d73e-25670e62525e"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Predict and transform predictions into binary arrays via argmax. \n",
        "predictions = model.predict(test_seq)\n",
        "prediction_indexes = [indexed_map[i] for i in np.argmax(predictions, axis=-1)]\n",
        "\n",
        "print(f\"Classification report: \\n{classification_report(test_labels, prediction_indexes, digits=3)}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.476     0.360     0.410       136\n",
            "           1      0.547     0.571     0.559       133\n",
            "           2      0.000     0.000     0.000         3\n",
            "           3      0.165     0.265     0.203        68\n",
            "           4      0.000     0.000     0.000         6\n",
            "           5      0.000     0.000     0.000         5\n",
            "\n",
            "   micro avg      0.407     0.407     0.407       351\n",
            "   macro avg      0.198     0.199     0.195       351\n",
            "weighted avg      0.423     0.407     0.410       351\n",
            " samples avg      0.407     0.407     0.407       351\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}